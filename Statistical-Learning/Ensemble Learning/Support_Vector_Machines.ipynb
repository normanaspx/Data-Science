{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Support Vector Machines.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVzXK13kEp7d"
      },
      "source": [
        "# Support vector machines - Norman Vicente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buExOIZkFDh_"
      },
      "source": [
        "SVM es un modelo para problemas de clasificacion, puede resolver problemas lineales y no lineales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-zwHOi1FM-r"
      },
      "source": [
        "## Idea principal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4aNrw5GFPaz"
      },
      "source": [
        "La idea principal es simple es crear un linea o hiperplano que separe a los datos en clases. \n",
        "\n",
        "Un hiperplano en un espacio euclidiano n-dimensional en un subconjunto plano, n-1 dimensional de ese espacio que divide el espacio en dos partes desconectadas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weyp-LyhEwlb"
      },
      "source": [
        "## Hipotesis SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oxbTsp-F0Ir"
      },
      "source": [
        "La hipotesis de SVM esta basado en la ecuacion de regresion lineal pero cuando no es lineal esta sufre una variacion a hiperplano:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/1280/1*HKIhqGG2-mpNCZ52O4kVmA.jpeg\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pu_YS66YGF0-"
      },
      "source": [
        "## Funcion de costo\n",
        "\n",
        "- C es un hiperparametro (Inversa de la fuerza de regularización.)\n",
        "- A medida que el valor de 'c' disminuye, el modelo no se adapta\n",
        "- A medida que aumenta el valor de 'c', el modelo se sobreajusta.\n",
        "- El primer termino esta asociado al máximo margen al minimizar la magnitud de el parámetro de pesos\n",
        "- El segundo término:  penaliza observaciones que no quedaron en el lado correcto. Usando la recta o hiperplano(hipotesis como frontera de decision)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmljeVT5GIf6"
      },
      "source": [
        "<img src=\"https://miro.medium.com/max/2000/1*AsCqMAkwy9m0eWmSqITolg.png\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGsLnbYKHk08"
      },
      "source": [
        "## Algoritmo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clg0UKNeHu-X"
      },
      "source": [
        "- SVM se basa en la idea de encontrar un hiperplano que separe mejor las características en diferentes dominios.\n",
        "- Los puntos más cercanos al hiperplano se denominan puntos del vector de soporte y la distancia de los vectores al hiperplano se denominan márgenes\n",
        "- Cuanto más alejados vectores de soporte del hiperplano, mayor es la probabilidad de clasificar correctamente los puntos en sus respectivas regiones o clases\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhxN6KEhINqj"
      },
      "source": [
        "## Propiedades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqtS67BUIPyO"
      },
      "source": [
        "### Hard margin SVM\n",
        "\n",
        "Entonces podemos ver que si los puntos son linealmente separables, entonces el hiperplano puede distinguir entre ellos y si se introduce algún valor atípico, entonces no es capaz de separarlos. Entonces, este tipo de SVM se denomina Hard margin SVM.\n",
        "\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/687/1*doKKm0KlPusiazXs-W8Mvg.png\" />\n",
        "\n",
        "### Soft margin SVM\n",
        "\n",
        "Básicamente, consideramos que los datos son linealmente separables y este podría no ser el caso en el escenario de la vida real. Necesitamos una actualización para que nuestra función pueda omitir algunos valores atípicos y pueda clasificar puntos separables casi linealmente.\n",
        "\n",
        "Se hace una modificacion a la hipotesis: \n",
        "<img src=\"https://miro.medium.com/max/183/1*0N2rw2v2UFFGyjggQ5CxHw.png\"/>\n",
        "\n",
        "si ξi = 0,\n",
        "los puntos pueden considerarse correctamente clasificados.\n",
        "demás:\n",
        "ξi> 0, puntos clasificados incorrectamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gnw7vMnYJiZg"
      },
      "source": [
        "## Kernel Trick"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KShnjQUK66I"
      },
      "source": [
        "El kernel es una forma de calcular el producto escalar de dos vectores x y Y en algún espacio de características (muy alto dimensional), por lo que las funciones del núcleo se denominan a veces \"generalizacion del producto punto\". Aplicar el kernel trick significa simplemente reemplazar el producto escalar de dos vectores por la función del kernel.\n",
        "\n",
        "Tipo de kernel:\n",
        "\n",
        "* Lineal\n",
        "* Polinomial\n",
        "* RBF / Nucleo gaussiano\n",
        "\n",
        "### Kernel polinomial\n",
        "Simplemente calculamos el producto escalar aumentando la potencia del núcleo. <br>\n",
        "<img src=\"https://miro.medium.com/max/321/1*Tt5V_m9iIwXc1xYoDLYmCA.png\" />\n",
        "\n",
        "### Kernel Gaussiano\n",
        "El kernel RBF es una función cuyo valor depende de la distancia desde el origen o desde algún punto. Se utiliza en los modelos SVM para obtener más información.\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/336/1*jTU-kuAWMnMMYwBWj8mTVw.png\" />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjN9fSqjL7tM"
      },
      "source": [
        "## Ventajas\n",
        "\n",
        "- Es realmente eficaz en dimensiones grandes.\n",
        "- Efectivo cuando el número de caracteristicas es más que ejemplos de entrenamiento.\n",
        "\n",
        "## Desventajas\n",
        "\n",
        "- Seleccionar la función de kernel adecuada puede ser complicado\n",
        "- Para conjuntos de datos más grandes, requiere una gran cantidad de tiempo para procesarse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjvFWIDbMZ_k"
      },
      "source": [
        "Referencias:\n",
        "- https://i.ytimg.com/vi/efR1C6CvhmE/hq720.jpg?sqp=-oaymwEcCOgCEMoBSFXyq4qpAw4IARUAAIhCGAFwAcABBg==&rs=AOn4CLAGwwF9uYqr7oM0gpTM0Gqqwbeufg\n",
        "\n",
        "- https://www.youtube.com/watch?v=pLLlX0juXGo&t=442s\n",
        "\n",
        "- https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989\n",
        "\n"
      ]
    }
  ]
}